#!/usr/bin/env python3
"""
æµ‹è¯•benchmark evaluationçš„é…ç½®å’Œé‡‡æ ·é€»è¾‘
"""

import sys
from pathlib import Path

#!/usr/bin/env python3

import torch
import torch.utils.data
from collections import defaultdict

def test_benchmark_sampling_logic():
    """Test the benchmark sampling logic."""
    
    # Mock benchmark files and sizes
    benchmark_files = [
        '/path/to/math.parquet',
        '/path/to/gsm8k.parquet', 
        '/path/to/hellaswag.parquet'
    ]
    
    # Mock dataset sizes
    benchmark_sizes = {
        'math': 150,
        'gsm8k': 80,
        'hellaswag': 200
    }
    
    max_samples_per_benchmark = 100
    
    print("=== Testing Benchmark Sampling Logic ===")
    print(f"Max samples per benchmark: {max_samples_per_benchmark}")
    print()
    
    # Simulate the actual logic from the code
    benchmark_datasets = []
    total_samples = 0
    
    for benchmark_file in benchmark_files:
        benchmark_name = benchmark_file.split('/')[-1].split('.')[0]
        benchmark_size = benchmark_sizes[benchmark_name]
        
        print(f"Processing {benchmark_name}:")
        print(f"  Original size: {benchmark_size}")
        
        # Apply per-benchmark sampling limit
        if max_samples_per_benchmark and benchmark_size > max_samples_per_benchmark:
            # Create indices for limited sampling
            indices = torch.randperm(benchmark_size)[:max_samples_per_benchmark]
            actual_size = max_samples_per_benchmark
            print(f"  Limited to: {actual_size} samples")
            print(f"  Sample indices: {indices[:10].tolist()}... (showing first 10)")
        else:
            actual_size = benchmark_size
            print(f"  No limiting needed: {actual_size} samples")
        
        # Mock creating the dataset
        benchmark_datasets.append(f"MockDataset_{benchmark_name}_{actual_size}")
        total_samples += actual_size
        print()
    
    print(f"Total samples across all benchmarks: {total_samples}")
    print(f"Expected total: {min(sum(benchmark_sizes.values()), max_samples_per_benchmark * len(benchmark_files))}")
    
    # Verify the logic
    expected_individual_samples = [
        min(benchmark_sizes['math'], max_samples_per_benchmark),
        min(benchmark_sizes['gsm8k'], max_samples_per_benchmark), 
        min(benchmark_sizes['hellaswag'], max_samples_per_benchmark)
    ]
    expected_total = sum(expected_individual_samples)
    
    print("\n=== Verification ===")
    print(f"Expected samples per benchmark: {expected_individual_samples}")
    print(f"Expected total: {expected_total}")
    print(f"Actual total: {total_samples}")
    print(f"Logic correct: {total_samples == expected_total}")
    
    # Test with different limits
    print("\n=== Testing Different Limits ===")
    for limit in [50, 100, 200, None]:
        print(f"\nLimit: {limit}")
        total = 0
        for name, size in benchmark_sizes.items():
            if limit is None:
                actual = size
            else:
                actual = min(size, limit)
            total += actual
            print(f"  {name}: {actual}/{size}")
        print(f"  Total: {total}")

if __name__ == "__main__":
    test_benchmark_sampling_logic()

def test_config_structure():
    """æµ‹è¯•é…ç½®æ–‡ä»¶ç»“æ„"""
    print("\nğŸ” æµ‹è¯•é…ç½®æ–‡ä»¶ç»“æ„...")
    
    import yaml
    config_path = Path("absolute_zero_reasoner/configs/azr_ppo_trainer_general.yaml")
    
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return False
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ£€æŸ¥benchmark_evaluationé…ç½®
    benchmark_config = config.get('benchmark_evaluation', {})
    if not benchmark_config:
        print("âŒ ç¼ºå°‘benchmark_evaluationé…ç½®")
        return False
    
    enabled = benchmark_config.get('enabled', False)
    if not enabled:
        print("âŒ benchmark_evaluationæœªå¯ç”¨")
        return False
    
    datasets = benchmark_config.get('datasets', [])
    if not datasets:
        print("âŒ æ²¡æœ‰é…ç½®datasets")
        return False
    
    frequency = benchmark_config.get('frequency', 0)
    if frequency <= 0:
        print("âŒ frequencyåº”è¯¥å¤§äº0")
        return False
        
    max_samples = benchmark_config.get('max_samples_per_benchmark')
    if max_samples is None:
        print("âš ï¸  å»ºè®®è®¾ç½®max_samples_per_benchmark")
    else:
        print(f"âœ… max_samples_per_benchmark: {max_samples}")
    
    print("âœ… benchmark_evaluationé…ç½®æ­£ç¡®")
    return True

def test_imports():
    """æµ‹è¯•å¯¼å…¥æ˜¯å¦æ­£ç¡®"""
    print("\nğŸ” æµ‹è¯•å¯¼å…¥...")
    
    try:
        from absolute_zero_reasoner.rewards.reward_managers import BenchmarkEvaluationRewardManager
        print("âœ… BenchmarkEvaluationRewardManagerå¯¼å…¥æˆåŠŸ")
        
        from absolute_zero_reasoner.trainer.ppo.reason_rl_ray_trainer import ReasonRLRayPPOTrainer
        print("âœ… ReasonRLRayPPOTrainerå¯¼å…¥æˆåŠŸ")
        
        from absolute_zero_reasoner.utils.benchmark_config import BenchmarkConfig
        print("âœ… BenchmarkConfigå¯¼å…¥æˆåŠŸ")
        
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def main():
    print("=== Benchmark Evaluationé‡‡æ ·é€»è¾‘æµ‹è¯• ===\n")
    
    # æµ‹è¯•å¯¼å…¥
    imports_ok = test_imports()
    
    # æµ‹è¯•é…ç½®
    config_ok = test_config_structure()
    
    # æµ‹è¯•é‡‡æ ·é€»è¾‘
    sampling_ok = test_benchmark_sampling_logic()
    
    if imports_ok and config_ok and sampling_ok:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\né…ç½®æ€»ç»“:")
        print("  - æ¯ä¸ªbenchmarkå°†ç‹¬ç«‹é™åˆ¶æ ·æœ¬æ•°")
        print("  - æ€»æ ·æœ¬æ•° = max_samples_per_benchmark Ã— benchmarkæ•°é‡")
        print("  - ä½¿ç”¨BenchmarkEvaluationRewardManagerè¿›è¡Œè¯„ä¼°")
        print("\næ³¨æ„äº‹é¡¹:")
        print("  - å½“å‰å®ç°å…ˆåŠ è½½æ‰€æœ‰benchmarkå†é™åˆ¶æ€»æ•°")
        print("  - æœªæ¥å¯ä»¥å®ç°çœŸæ­£çš„per-benchmarké‡‡æ ·")
        return True
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
